{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from cassandra.cluster import Cluster\n",
    "from cassandra.query import BatchStatement\n",
    "from cassandra.auth import PlainTextAuthProvider\n",
    "from cassandra import WriteTimeout\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "import csv\n",
    "\n",
    "from fredapi import Fred\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "import yfinance as yf\n",
    "from datetime import datetime, timedelta\n",
    "import time\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load ticker symbols from nasdaq screener csv file\n",
    "def load_tickers(filepath):\n",
    "    df = pd.read_csv(filepath)\n",
    "    return df.iloc[:, 0].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fetch historical data for a given ticker\n",
    "def fetch_stock_data(ticker):\n",
    "    # Calculate the date 5 years ago from today\n",
    "    end_date = datetime.now()\n",
    "    start_date = end_date - timedelta(days=365*5)\n",
    "    \n",
    "    # Fetch historical data\n",
    "    data = yf.download(ticker, start=start_date.strftime('%Y-%m-%d'), end=end_date.strftime('%Y-%m-%d'))\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load ticker symbols\n",
    "tickers = load_tickers('nasdaq_screener.csv')\n",
    "\n",
    "# Create a subfolder for the financial data\n",
    "output_dir = 'fin_data'\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Dictionary to hold data for all tickers\n",
    "all_data = {}\n",
    "\n",
    "# Loop through each ticker and fetch its data\n",
    "index = 0\n",
    "for ticker in tickers:\n",
    "    try:\n",
    "        data = fetch_stock_data(ticker)\n",
    "        all_data[ticker] = data\n",
    "        # Save data\n",
    "        data.to_csv(os.path.join(output_dir, f'{ticker}_5_years_data.csv'))\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to fetch data for {ticker}: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the directory containing the CSV files\n",
    "source_dir = os.path.expanduser('fin_data/')\n",
    "\n",
    "# List all files in the directory\n",
    "files = os.listdir(source_dir)\n",
    "\n",
    "# Loop through each file\n",
    "for file in files:\n",
    "    file_path = os.path.join(source_dir, file)\n",
    "    \n",
    "    # Check if the file is a CSV file\n",
    "    if file.endswith('.csv'):\n",
    "        try:\n",
    "            # Load the CSV file\n",
    "            df = pd.read_csv(file_path)\n",
    "            \n",
    "            # Check if the file is empty or contains NA values\n",
    "            if df.empty or df.isna().any().any():\n",
    "                os.remove(file_path)  # Remove the file if it's empty or contains NA\n",
    "                print(f\"Deleted '{file}' because it was empty or contained NA values.\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {file}: {e}\")\n",
    "\n",
    "print(\"Cleanup complete.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_schema(session):\n",
    "    # CQL to create a keyspace\n",
    "    session.execute(\"\"\"\n",
    "        CREATE KEYSPACE IF NOT EXISTS fin_data WITH replication = {\n",
    "            'class': 'SimpleStrategy', \n",
    "            'replication_factor': 1\n",
    "        }\n",
    "    \"\"\")\n",
    "\n",
    "    # CQL to use the keyspace\n",
    "    session.execute(\"USE fin_data\")\n",
    "\n",
    "    # CQL to create a table\n",
    "    session.execute(\"\"\"\n",
    "        CREATE TABLE IF NOT EXISTS ohlcv (\n",
    "            symbol text,\n",
    "            date timestamp,\n",
    "            open double,\n",
    "            high double,\n",
    "            low double,\n",
    "            close double,\n",
    "            adj_close double,\n",
    "            volume bigint,\n",
    "            PRIMARY KEY (symbol, date)\n",
    "        )\n",
    "    \"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_date(date_str):\n",
    "    return datetime.strptime(date_str, '%Y-%m-%d')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def execute_with_retries(session, batch, max_retries=5):\n",
    "    retries = 0\n",
    "    backoff_time = 2  # Start with 2 seconds backoff time\n",
    "    while retries < max_retries:\n",
    "        try:\n",
    "            session.execute(batch)\n",
    "            break  # Break the loop if execution is successful\n",
    "        except WriteTimeout:\n",
    "            print(f\"WriteTimeout: retrying {retries+1}/{max_retries} after {backoff_time}s...\")\n",
    "            time.sleep(backoff_time)  # Sleep for backoff_time seconds before retrying\n",
    "            backoff_time *= 2  # Double the backoff time for the next retry\n",
    "            retries += 1\n",
    "    if retries == max_retries:\n",
    "        print(\"Failed to execute batch after several retries.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(session, file_path, symbol):\n",
    "    batch = BatchStatement()\n",
    "    batch_size = 0  # Track the size of the batch\n",
    "\n",
    "    with open(file_path, newline='') as csvfile:\n",
    "        reader = csv.DictReader(csvfile)\n",
    "        for row in reader:\n",
    "            batch.add(\n",
    "                \"\"\"\n",
    "                INSERT INTO fin_data.ohlcv (symbol, date, open, high, low, close, adj_close, volume)\n",
    "                VALUES (%s, %s, %s, %s, %s, %s, %s, %s)\n",
    "                \"\"\",\n",
    "                (symbol, parse_date(row['Date']), float(row['Open']), float(row['High']),\n",
    "                 float(row['Low']), float(row['Close']), float(row['Adj Close']), int(row['Volume']))\n",
    "            )\n",
    "            batch_size += 1\n",
    "            if batch_size >= 100:  # Execute batch after collecting 50 statements\n",
    "                execute_with_retries(session, batch)\n",
    "                batch = BatchStatement()  # Reset batch after execution\n",
    "                batch_size = 0  # Reset batch size\n",
    "\n",
    "        if batch_size > 0:  # Ensure any remaining statements are executed\n",
    "            execute_with_retries(session, batch)\n",
    "\n",
    "        print(f\"Data from {symbol} loaded successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_table(keyspace_name, table_name):\n",
    "    \"\"\" Drop a table in a given keyspace \"\"\"\n",
    "    # Set up connection parameters\n",
    "    auth_provider = PlainTextAuthProvider(username='cassandra', password='cassandra')  # Update credentials if needed\n",
    "    cluster = Cluster(['127.0.0.1'], auth_provider=auth_provider)  # Adjust IP if Cassandra is hosted elsewhere\n",
    "    session = cluster.connect(keyspace_name)  # Connect to the specified keyspace\n",
    "\n",
    "    # Drop table\n",
    "    try:\n",
    "        session.execute(f\"DROP TABLE IF EXISTS {table_name};\")\n",
    "        print(f\"Table {table_name} has been dropped successfully.\")\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred while dropping the table: {e}\")\n",
    "    finally:\n",
    "        # Clean up, close the session and cluster connection\n",
    "        session.shutdown()\n",
    "        cluster.shutdown()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_symbol_data(symbol, start_date, end_date):\n",
    "    \n",
    "    cluster = Cluster(['127.0.0.1'])  # Adjust if Cassandra is hosted elsewhere\n",
    "    session = cluster.connect('fin_data')\n",
    "\n",
    "    # Prepare the query\n",
    "    query = \"\"\"\n",
    "        SELECT * FROM ohlcv WHERE symbol = %s AND date >= %s AND date <= %s;\n",
    "    \"\"\"\n",
    "    # Convert string dates to datetime objects\n",
    "    start_dt = datetime.strptime(start_date, '%Y-%m-%d')\n",
    "    end_dt = datetime.strptime(end_date, '%Y-%m-%d')\n",
    "\n",
    "    # Execute the query\n",
    "    try:\n",
    "        rows = session.execute(query, (symbol, start_dt, end_dt))\n",
    "        for row in rows:\n",
    "            print(row)\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "    finally:\n",
    "        # Clean up, close the session and cluster connection\n",
    "        session.shutdown()\n",
    "        cluster.shutdown()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_table():\n",
    "    # Connect to Cassandra\n",
    "    # Adjust the connection settings as needed for your setup\n",
    "    auth_provider = PlainTextAuthProvider(username='cassandra', password='cassandra')  # Update with actual credentials\n",
    "    cluster = Cluster(['127.0.0.1'], auth_provider=auth_provider)\n",
    "    session = cluster.connect()\n",
    "\n",
    "    # Create the schema\n",
    "    create_schema(session)\n",
    "\n",
    "    print(\"Keyspace and table created successfully.\")\n",
    "\n",
    "    # Clean up\n",
    "    session.shutdown()\n",
    "    cluster.shutdown()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:cassandra.cluster:Cluster.__init__ called with contact_points specified, but no load_balancing_policy. In the next major version, this will raise an error; please specify a load-balancing policy. (contact_points = ['127.0.0.1'], lbp = None)\n",
      "WARNING:cassandra.cluster:Downgrading core protocol version from 66 to 65 for 127.0.0.1:9042. To avoid this, it is best practice to explicitly set Cluster(protocol_version) to the version supported by your cluster. http://datastax.github.io/python-driver/api/cassandra/cluster.html#cassandra.cluster.Cluster.protocol_version\n",
      "WARNING:cassandra.cluster:Downgrading core protocol version from 65 to 5 for 127.0.0.1:9042. To avoid this, it is best practice to explicitly set Cluster(protocol_version) to the version supported by your cluster. http://datastax.github.io/python-driver/api/cassandra/cluster.html#cassandra.cluster.Cluster.protocol_version\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keyspace and table created successfully.\n"
     ]
    }
   ],
   "source": [
    "create_table()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:cassandra.cluster:Cluster.__init__ called with contact_points specified, but no load_balancing_policy. In the next major version, this will raise an error; please specify a load-balancing policy. (contact_points = ['127.0.0.1'], lbp = None)\n",
      "WARNING:cassandra.cluster:Downgrading core protocol version from 66 to 65 for 127.0.0.1:9042. To avoid this, it is best practice to explicitly set Cluster(protocol_version) to the version supported by your cluster. http://datastax.github.io/python-driver/api/cassandra/cluster.html#cassandra.cluster.Cluster.protocol_version\n",
      "WARNING:cassandra.cluster:Downgrading core protocol version from 65 to 5 for 127.0.0.1:9042. To avoid this, it is best practice to explicitly set Cluster(protocol_version) to the version supported by your cluster. http://datastax.github.io/python-driver/api/cassandra/cluster.html#cassandra.cluster.Cluster.protocol_version\n",
      "  0%|          | 1/6638 [00:00<1:31:24,  1.21it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data from PEBK loaded successfully.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 2/6638 [00:01<1:28:25,  1.25it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data from ARR loaded successfully.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 3/6638 [00:01<1:07:15,  1.64it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data from GCTS loaded successfully.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 4/6638 [00:02<1:04:35,  1.71it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data from HYFM loaded successfully.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 5/6638 [00:03<1:07:54,  1.63it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data from ALVR loaded successfully.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 6/6638 [00:03<1:05:02,  1.70it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data from CLBT loaded successfully.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 8/6638 [00:04<47:20,  2.33it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data from FA loaded successfully.\n",
      "Data from ZURA loaded successfully.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 9/6638 [00:05<1:00:28,  1.83it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data from ANTE loaded successfully.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 10/6638 [00:05<1:08:37,  1.61it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data from GORO loaded successfully.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 11/6638 [00:06<1:14:18,  1.49it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data from SEEL loaded successfully.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 12/6638 [00:07<1:18:41,  1.40it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data from LDOS loaded successfully.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 13/6638 [00:08<1:21:07,  1.36it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data from MGA loaded successfully.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 14/6638 [00:08<1:14:09,  1.49it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data from SNTI loaded successfully.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 15/6638 [00:09<1:18:20,  1.41it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data from PXS loaded successfully.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 16/6638 [00:10<1:20:47,  1.37it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data from PRMW loaded successfully.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 17/6638 [00:10<1:05:27,  1.69it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data from YOSH loaded successfully.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 18/6638 [00:11<1:12:03,  1.53it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data from OSUR loaded successfully.\n",
      "Data from GROMW loaded successfully.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 20/6638 [00:12<58:27,  1.89it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data from FNLC loaded successfully.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 21/6638 [00:15<2:03:44,  1.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data from PKG loaded successfully.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 22/6638 [00:15<1:44:36,  1.05it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data from KRNL loaded successfully.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 23/6638 [00:16<1:36:08,  1.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data from BILL loaded successfully.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 24/6638 [00:17<1:32:38,  1.19it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data from AFBI loaded successfully.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 25/6638 [00:17<1:30:36,  1.22it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data from JNJ loaded successfully.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 26/6638 [00:18<1:29:13,  1.24it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data from CHRS loaded successfully.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 27/6638 [00:19<1:27:34,  1.26it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data from TRAW loaded successfully.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 28/6638 [00:20<1:26:59,  1.27it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data from APLS loaded successfully.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 29/6638 [00:20<1:26:19,  1.28it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data from TDC loaded successfully.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 30/6638 [00:21<1:25:23,  1.29it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data from NYMT loaded successfully.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 31/6638 [00:22<1:24:05,  1.31it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data from OMAB loaded successfully.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 32/6638 [00:22<1:10:25,  1.56it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data from ALVO loaded successfully.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 33/6638 [00:23<1:12:04,  1.53it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data from SMTC loaded successfully.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 34/6638 [00:24<1:21:23,  1.35it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data from GROM loaded successfully.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 35/6638 [00:25<1:22:02,  1.34it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data from BBVA loaded successfully.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 36/6638 [00:25<1:06:57,  1.64it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data from TNON loaded successfully.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 38/6638 [00:26<48:24,  2.27it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data from HTOO loaded successfully.\n",
      "Data from VSTS loaded successfully.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 39/6638 [00:26<57:43,  1.91it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data from WU loaded successfully.\n",
      "Data from GMFIW loaded successfully.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 41/6638 [00:27<49:00,  2.24it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data from NTNX loaded successfully.\n",
      "Data from MNDR loaded successfully.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 43/6638 [00:27<37:56,  2.90it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data from JTAI loaded successfully.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 44/6638 [00:28<46:42,  2.35it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data from GAME loaded successfully.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 45/6638 [00:29<54:01,  2.03it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data from PZZA loaded successfully.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 47/6638 [00:30<47:29,  2.31it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data from MSN loaded successfully.\n",
      "Data from GLADZ loaded successfully.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 48/6638 [00:30<55:45,  1.97it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data from BCSF loaded successfully.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 49/6638 [00:31<1:02:22,  1.76it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data from LOMA loaded successfully.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 50/6638 [00:32<1:02:40,  1.75it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data from FRES loaded successfully.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 51/6638 [00:33<1:13:53,  1.49it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data from WMPN loaded successfully.\n",
      "Data from AIMDW loaded successfully.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 53/6638 [00:33<59:37,  1.84it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data from WKC loaded successfully.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 54/6638 [00:34<1:04:38,  1.70it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data from JEF loaded successfully.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 55/6638 [00:35<1:07:48,  1.62it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data from GTE loaded successfully.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 56/6638 [00:35<1:10:43,  1.55it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data from WFCF loaded successfully.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 57/6638 [00:36<1:12:18,  1.52it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data from FFIV loaded successfully.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 58/6638 [00:37<1:13:08,  1.50it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data from NDAQ loaded successfully.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 60/6638 [00:38<57:45,  1.90it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data from CPT loaded successfully.\n",
      "Data from CWD loaded successfully.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 61/6638 [00:38<1:03:01,  1.74it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data from SXTC loaded successfully.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 62/6638 [00:39<1:06:44,  1.64it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data from RENB loaded successfully.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 63/6638 [00:40<1:09:54,  1.57it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data from EFSC loaded successfully.\n",
      "Data from ONFOW loaded successfully.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 65/6638 [00:41<55:52,  1.96it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data from MIN loaded successfully.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 66/6638 [00:41<52:35,  2.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data from HTZ loaded successfully.\n",
      "Data from OCSAW loaded successfully.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 68/6638 [00:42<51:39,  2.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data from NERV loaded successfully.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 69/6638 [00:43<57:22,  1.91it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data from FIVN loaded successfully.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 70/6638 [00:43<1:03:48,  1.72it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data from XPL loaded successfully.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 71/6638 [00:44<1:06:57,  1.63it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data from SCL loaded successfully.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 72/6638 [00:45<1:10:30,  1.55it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data from EURN loaded successfully.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 73/6638 [00:45<1:04:04,  1.71it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data from JZXN loaded successfully.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 74/6638 [00:46<56:02,  1.95it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data from DRCT loaded successfully.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 75/6638 [00:46<56:46,  1.93it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data from LI loaded successfully.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 76/6638 [00:47<1:02:47,  1.74it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data from MTRN loaded successfully.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 77/6638 [00:47<1:00:00,  1.82it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data from OPT loaded successfully.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 78/6638 [00:48<1:03:36,  1.72it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data from CHPT loaded successfully.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 80/6638 [00:49<52:39,  2.08it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data from INLX loaded successfully.\n",
      "Data from CLCO loaded successfully.\n",
      "Data from SDAWW loaded successfully.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 82/6638 [00:50<47:06,  2.32it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data from PIM loaded successfully.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|▏         | 83/6638 [00:50<1:00:16,  1.81it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data from TXT loaded successfully.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|▏         | 84/6638 [00:51<52:15,  2.09it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data from SCCG loaded successfully.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|▏         | 85/6638 [00:51<51:04,  2.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data from LDTC loaded successfully.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|▏         | 86/6638 [00:51<45:12,  2.42it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data from ILAG loaded successfully.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|▏         | 87/6638 [00:52<54:08,  2.02it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data from BGR loaded successfully.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|▏         | 88/6638 [00:53<59:50,  1.82it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data from OPXS loaded successfully.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|▏         | 89/6638 [00:54<1:05:36,  1.66it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data from BROG loaded successfully.\n",
      "Data from STSSW loaded successfully.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|▏         | 91/6638 [00:54<53:30,  2.04it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data from PAAS loaded successfully.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|▏         | 92/6638 [00:55<59:09,  1.84it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data from KMDA loaded successfully.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|▏         | 93/6638 [00:56<1:03:07,  1.73it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data from VEV loaded successfully.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|▏         | 94/6638 [00:56<1:06:50,  1.63it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data from THRY loaded successfully.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|▏         | 95/6638 [00:57<58:19,  1.87it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data from MTEK loaded successfully.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|▏         | 96/6638 [00:57<1:04:00,  1.70it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data from CECO loaded successfully.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|▏         | 97/6638 [00:58<1:07:33,  1.61it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data from UBX loaded successfully.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|▏         | 98/6638 [00:59<1:09:59,  1.56it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data from FHTX loaded successfully.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|▏         | 99/6638 [00:59<1:11:52,  1.52it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data from ADUS loaded successfully.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 100/6638 [01:00<1:13:07,  1.49it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data from YUM loaded successfully.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 101/6638 [01:01<1:06:11,  1.65it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data from EMKR loaded successfully.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "cluster = Cluster(['127.0.0.1'])  # Adjust if Cassandra is hosted elsewhere\n",
    "session = cluster.connect()\n",
    "\n",
    "i = 0\n",
    "data_dir = '/home/xca7/Desktop/Storage-Solutions-for-Big-Data-CA1/fin_data'\n",
    "for filename in tqdm(os.listdir(data_dir)):\n",
    "    if i > 100:\n",
    "        break\n",
    "    i += 1\n",
    "    if filename.endswith('.csv'):\n",
    "        symbol = filename.split('_')[0]  # Extract the symbol from the filename\n",
    "        load_data(session, os.path.join(data_dir, filename), symbol)\n",
    "\n",
    "session.shutdown()\n",
    "cluster.shutdown()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:cassandra.cluster:Cluster.__init__ called with contact_points specified, but no load_balancing_policy. In the next major version, this will raise an error; please specify a load-balancing policy. (contact_points = ['127.0.0.1'], lbp = None)\n",
      "WARNING:cassandra.cluster:Downgrading core protocol version from 66 to 65 for 127.0.0.1:9042. To avoid this, it is best practice to explicitly set Cluster(protocol_version) to the version supported by your cluster. http://datastax.github.io/python-driver/api/cassandra/cluster.html#cassandra.cluster.Cluster.protocol_version\n",
      "WARNING:cassandra.cluster:Downgrading core protocol version from 65 to 5 for 127.0.0.1:9042. To avoid this, it is best practice to explicitly set Cluster(protocol_version) to the version supported by your cluster. http://datastax.github.io/python-driver/api/cassandra/cluster.html#cassandra.cluster.Cluster.protocol_version\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Table ohlcv has been dropped successfully.\n"
     ]
    }
   ],
   "source": [
    "#drop_table('fin_data', 'ohlcv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_spark_session():\n",
    "    spark = SparkSession.builder \\\n",
    "        .appName(\"Cassandra Integration Example\") \\\n",
    "        .config(\"spark.cassandra.connection.host\", \"localhost\") \\\n",
    "        .config(\"spark.jars.packages\", \"com.datastax.spark:spark-cassandra-connector_2.12:3.5.0\") \\\n",
    "        .getOrCreate()\n",
    "    return spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Spark session\n",
    "spark = create_spark_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def list_unique_symbols(spark, keyspace, table):\n",
    "    # Load the data from Cassandra into a DataFrame\n",
    "    df = spark.read \\\n",
    "        .format(\"org.apache.spark.sql.cassandra\") \\\n",
    "        .options(keyspace=keyspace, table=table) \\\n",
    "        .load()\n",
    "    \n",
    "    # Select distinct symbols and collect them into a list\n",
    "    symbols = df.select(\"symbol\").distinct().collect()\n",
    "    \n",
    "    # Extract symbols from rows\n",
    "    symbol_list = [row['symbol'] for row in symbols]\n",
    "    print(\"Unique symbols:\", symbol_list)\n",
    "    return symbol_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 13:====================================================>   (13 + 1) / 14]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique symbols: ['ARR', 'PEBK', 'HYFM', 'ZURA', 'CLBT', 'GORO', 'ANTE', 'GCTS', 'ALVR', 'SEEL', 'FA']\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "keyspace = 'fin_data'\n",
    "table = 'ohlcv'\n",
    "list_unique_symbols(spark, keyspace, table)\n",
    "print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_symbol_data(spark, keyspace, table, symbol, start_date, end_date):\n",
    "    # Load data from the specified table and keyspace\n",
    "    df = spark.read \\\n",
    "        .format(\"org.apache.spark.sql.cassandra\") \\\n",
    "        .options(keyspace=keyspace, table=table) \\\n",
    "        .load()\n",
    "\n",
    "    # Filter data for the specified symbol and date range\n",
    "    filtered_df = df.filter(\n",
    "        (col(\"symbol\") == symbol) & \n",
    "        (col(\"date\") >= start_date) & \n",
    "        (col(\"date\") <= end_date)\n",
    "    )\n",
    "\n",
    "    # Convert to Pandas DataFrame\n",
    "    pandas_df = filtered_df.toPandas()\n",
    "    return pandas_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>symbol</th>\n",
       "      <th>date</th>\n",
       "      <th>adj_close</th>\n",
       "      <th>close</th>\n",
       "      <th>high</th>\n",
       "      <th>low</th>\n",
       "      <th>open</th>\n",
       "      <th>volume</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>PEBK</td>\n",
       "      <td>2019-04-23 01:00:00</td>\n",
       "      <td>25.188534</td>\n",
       "      <td>27.450001</td>\n",
       "      <td>27.450001</td>\n",
       "      <td>27.309999</td>\n",
       "      <td>27.309999</td>\n",
       "      <td>1600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>PEBK</td>\n",
       "      <td>2019-04-24 01:00:00</td>\n",
       "      <td>24.885725</td>\n",
       "      <td>27.120001</td>\n",
       "      <td>27.459999</td>\n",
       "      <td>26.750000</td>\n",
       "      <td>27.450001</td>\n",
       "      <td>2400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>PEBK</td>\n",
       "      <td>2019-04-25 01:00:00</td>\n",
       "      <td>24.968306</td>\n",
       "      <td>27.209999</td>\n",
       "      <td>27.750000</td>\n",
       "      <td>27.209999</td>\n",
       "      <td>27.750000</td>\n",
       "      <td>2300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>PEBK</td>\n",
       "      <td>2019-04-26 01:00:00</td>\n",
       "      <td>24.922428</td>\n",
       "      <td>27.160000</td>\n",
       "      <td>27.790001</td>\n",
       "      <td>26.900000</td>\n",
       "      <td>27.309999</td>\n",
       "      <td>138100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>PEBK</td>\n",
       "      <td>2019-04-29 01:00:00</td>\n",
       "      <td>25.142654</td>\n",
       "      <td>27.400000</td>\n",
       "      <td>27.730000</td>\n",
       "      <td>27.070000</td>\n",
       "      <td>27.070000</td>\n",
       "      <td>18200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1198</th>\n",
       "      <td>PEBK</td>\n",
       "      <td>2024-01-25 00:00:00</td>\n",
       "      <td>29.670000</td>\n",
       "      <td>29.670000</td>\n",
       "      <td>29.750000</td>\n",
       "      <td>29.540001</td>\n",
       "      <td>29.650000</td>\n",
       "      <td>3200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1199</th>\n",
       "      <td>PEBK</td>\n",
       "      <td>2024-01-26 00:00:00</td>\n",
       "      <td>29.799999</td>\n",
       "      <td>29.799999</td>\n",
       "      <td>29.959999</td>\n",
       "      <td>29.799999</td>\n",
       "      <td>29.959999</td>\n",
       "      <td>3700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1200</th>\n",
       "      <td>PEBK</td>\n",
       "      <td>2024-01-29 00:00:00</td>\n",
       "      <td>29.670000</td>\n",
       "      <td>29.670000</td>\n",
       "      <td>29.799999</td>\n",
       "      <td>29.500000</td>\n",
       "      <td>29.590000</td>\n",
       "      <td>6600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1201</th>\n",
       "      <td>PEBK</td>\n",
       "      <td>2024-01-30 00:00:00</td>\n",
       "      <td>29.570000</td>\n",
       "      <td>29.570000</td>\n",
       "      <td>29.879999</td>\n",
       "      <td>29.570000</td>\n",
       "      <td>29.610001</td>\n",
       "      <td>1600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1202</th>\n",
       "      <td>PEBK</td>\n",
       "      <td>2024-01-31 00:00:00</td>\n",
       "      <td>29.090000</td>\n",
       "      <td>29.090000</td>\n",
       "      <td>29.600000</td>\n",
       "      <td>29.090000</td>\n",
       "      <td>29.600000</td>\n",
       "      <td>1700</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1203 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     symbol                date  adj_close      close       high        low  \\\n",
       "0      PEBK 2019-04-23 01:00:00  25.188534  27.450001  27.450001  27.309999   \n",
       "1      PEBK 2019-04-24 01:00:00  24.885725  27.120001  27.459999  26.750000   \n",
       "2      PEBK 2019-04-25 01:00:00  24.968306  27.209999  27.750000  27.209999   \n",
       "3      PEBK 2019-04-26 01:00:00  24.922428  27.160000  27.790001  26.900000   \n",
       "4      PEBK 2019-04-29 01:00:00  25.142654  27.400000  27.730000  27.070000   \n",
       "...     ...                 ...        ...        ...        ...        ...   \n",
       "1198   PEBK 2024-01-25 00:00:00  29.670000  29.670000  29.750000  29.540001   \n",
       "1199   PEBK 2024-01-26 00:00:00  29.799999  29.799999  29.959999  29.799999   \n",
       "1200   PEBK 2024-01-29 00:00:00  29.670000  29.670000  29.799999  29.500000   \n",
       "1201   PEBK 2024-01-30 00:00:00  29.570000  29.570000  29.879999  29.570000   \n",
       "1202   PEBK 2024-01-31 00:00:00  29.090000  29.090000  29.600000  29.090000   \n",
       "\n",
       "           open  volume  \n",
       "0     27.309999    1600  \n",
       "1     27.450001    2400  \n",
       "2     27.750000    2300  \n",
       "3     27.309999  138100  \n",
       "4     27.070000   18200  \n",
       "...         ...     ...  \n",
       "1198  29.650000    3200  \n",
       "1199  29.959999    3700  \n",
       "1200  29.590000    6600  \n",
       "1201  29.610001    1600  \n",
       "1202  29.600000    1700  \n",
       "\n",
       "[1203 rows x 8 columns]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Example usage\n",
    "\n",
    "keyspace = 'fin_data'\n",
    "table = 'ohlcv'\n",
    "symbol = 'PEBK'  # Example symbol\n",
    "start_date = '2019-01-01'  # Example start date\n",
    "end_date = '2024-01-31'  # Example end date\n",
    "    \n",
    "query_symbol_data(spark, keyspace, table, symbol, start_date, end_date)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets construct some portfolios and see how they perform!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
